{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Lab Exercise 2\n",
    "\n",
    "**WARNING:** you must have finished the first exercise before this one as you will re-use parts of the code.\n",
    "\n",
    "In the first lab exercise, we built a simple linear classifier.\n",
    "Although it can give reasonable results on the MNIST datasetÂ (~92.5% of accuracy), deeper neural networks can achieve more the 99% accuracy.\n",
    "However, it can quickly become really impracical to explicitly code forward and backward passes.\n",
    "Hence, it is useful to rely on an auto-diff library where we specify the forward pass once, and the backward pass is automatically deduced from the computational graph structure.\n",
    "\n",
    "In this lab exercise, we will build a small and simple auto-diff lib that mimics the autograd mechanism from Pytorch (of course, we will simplify a lot!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs that we will use\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "# To load the data we will use the script of Gaetan Marceau Caron\n",
    "# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n",
    "import dataset_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mnist dataset\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "\t# this link doesn't work any more,\n",
    "\t# seach on google for the file \"mnist.pkl.gz\"\n",
    "\t# and download it\n",
    "\t!wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2682546dde0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 900\n",
    "label = train_data[1][index]\n",
    "picture = train_data[0][index]\n",
    "\n",
    "print(\"label: %i\" % label)\n",
    "plt.imshow(picture.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation nodes\n",
    "\n",
    "Instead of directly manipulating numpy arrays, we will manipulate abstraction that contains:\n",
    "- a value (i.e. a numpy array)\n",
    "- a bool indicating if we wish to compute the gradient with respect to the value\n",
    "- the gradient with respect to the value\n",
    "- the operation to call during backpropagation\n",
    "\n",
    "There will be two kind of nodes:\n",
    "- Tensor: a generic computation node\n",
    "- Parameter: a computation node that is used to store parameters of the network. Parameters are always leaf nodes, i.e. they cannot be build from other computation nodes.\n",
    "\n",
    "Our implementation of the backward pass will be really simple and incorrect in the general case (i.e. won't work with computation graph with loops).\n",
    "We will just apply the derivative function for a given tensor and then call the ones of its antecedents, recursively.\n",
    "This simple algorithm is good enough for this exercise.\n",
    "\n",
    "Note that a real implementation of backprop will store temporary values during forward that can be used during backward to improve computation speed. We do not do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\tdef __init__(self, data, require_grad = False):\n",
    "\t\t# test type of data: should be np array\n",
    "\t\tif isinstance(data, float):\n",
    "\t\t\tdata = np.array([data,])\n",
    "\t\tif type(data) != np.ndarray:\n",
    "\t\t\traise RuntimeError(\"Input should be a numpy array\")\n",
    "\n",
    "\t\t# store data for this tensor\n",
    "\t\tself.data = data\n",
    "\t\tself.require_grad = require_grad\n",
    "\n",
    "\t\t# this values should be set to enable autograd!\n",
    "\t\tself.gradient = None\n",
    "\t\tself.d = None\n",
    "\t\tself.backptr = None\n",
    "\n",
    "\tdef zero_grad(self):\n",
    "\t\t\"\"\"\n",
    "\t\tSet the gradient of thie tensor to 0\n",
    "\t\t\"\"\"\n",
    "\t\tif self.require_grad:\n",
    "\t\t\tself.gradient = np.zeros_like(self.data)\n",
    "\n",
    "\tdef accumulate_gradient(self, gradient):\n",
    "\t\t\"\"\"\n",
    "\t\tAccumulte gradient for this tensor\n",
    "\t\t\"\"\"\n",
    "\t\tif gradient.shape != self.data.shape:\n",
    "\t\t\traise RuntimeError(\"Invalid gradient dimension\")\n",
    "\n",
    "\t\tif self.gradient is None:\n",
    "\t\t\tself.gradient = np.copy(gradient)\n",
    "\t\telse:\n",
    "\t\t\tself.gradient += gradient\n",
    "\n",
    "\tdef backward(self, g = None):\n",
    "\t\t\"\"\"\n",
    "\t\tThe backward pass!\n",
    "\t\tIf g != None, then g is the gradient for the current node.\n",
    "\t\ti.e. g will be != None only for the loss output.\n",
    "\n",
    "\t\tYou should call the function stored in self.d with correct arguments,\n",
    "\t\tand then recursively call the backward methods of tensors in the backptr list if:\n",
    "\t\t1. they require a gradient\n",
    "\t\t2. they are of type Tensor: check with isinstance(o, Tensor)\n",
    "\t\t\"\"\"\n",
    "\t\tif not self.require_grad:  # stop right now if this node does not require a gradient\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tif g is not None:\n",
    "\t\t\tif isinstance(g, float):\n",
    "\t\t\t\tg = np.array([g])\n",
    "\t\t\tif type(g) != np.ndarray:\n",
    "\t\t\t\traise RuntimeError(\"Gradient should be a numpy array\")\n",
    "\t\t\tif g.shape != self.data.shape:\n",
    "\t\t\t\traise RuntimeError(\"Gradient of different size than the value!\")\n",
    "\n",
    "\t\t\tself.gradient = g\n",
    "\n",
    "\t\tself.d(self.backptr, self.gradient)\n",
    "\n",
    "\t\tfor ptr in self.backptr:\n",
    "\t\t\tif isinstance(ptr, Tensor) and ptr.require_grad and not isinstance(ptr, Parameter):\n",
    "\t\t\t\tptr.backward()\n",
    "\n",
    "\n",
    "class Parameter(Tensor):\n",
    "\t\"\"\"\n",
    "\tThis class will be used to store parameters of the network only!\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, data, name = \"unamed\"):\n",
    "\t\tsuper().__init__(data, require_grad = True)\n",
    "\t\tself.name = name\n",
    "\t\tself.velocity = 0.\n",
    "\n",
    "\tdef backward(self):\n",
    "\t\traise RuntimeError(\"You cannot backprop from a Parameter node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Functions manipulate tensors and build the required information for autograd.\n",
    "A function returns a Tensor that should have require_grad = True if at least of the arguments require a gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_require_grad(l):\n",
    "\t\"\"\"\n",
    "\tInput:\n",
    "\t- l: an iterable (e.g. a list)\n",
    "\tOuput:\n",
    "\t- True if any tensor in the input requires a gradient\n",
    "\t\"\"\"\n",
    "\treturn any(t.require_grad for t in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an exemple with the ReLU\n",
    "def relu(x):\n",
    "\tv = np.maximum(0, x.data)\n",
    "\n",
    "\toutput = Tensor(v, require_grad = x.require_grad)\n",
    "\toutput.d = backward_relu\n",
    "\toutput.backptr = [x]\n",
    "\n",
    "\treturn output\n",
    "\n",
    "def backward_relu(backptr, g):\n",
    "\tx, = backptr\n",
    "\n",
    "\t# the gradient is accumulated in the arguments only if required\n",
    "\tif x.require_grad:\n",
    "\t\tx.accumulate_gradient(g * (x.data > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "\tv = np.tanh(x.data)\n",
    "\n",
    "\toutput = Tensor(v, require_grad = x.require_grad)\n",
    "\toutput.d = backward_tanh\n",
    "\toutput.backptr = [x]\n",
    "\n",
    "\treturn output\n",
    "\n",
    "def backward_tanh(backptr, g):\n",
    "\tx, = backptr\n",
    "\n",
    "\tif x.require_grad:\n",
    "\t\tx.accumulate_gradient(g * (1 - np.tanh(x.data) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement the affine transform operation.\n",
    "You can reuse the code from the first lab exercise, with one major difference: you have to compute the gradient with respect to x too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_transform(W, b, x):\n",
    "\tv = W.data @ x.data + b.data\n",
    "\n",
    "\toutput = Tensor(v, require_grad = True)\n",
    "\toutput.d = backward_affine_transform\n",
    "\toutput.backptr = [W, b, x]\n",
    "\n",
    "\treturn output\n",
    "\n",
    "def backward_affine_transform(backptr, g):\n",
    "\tW, b, x = backptr\n",
    "\n",
    "\tif W.require_grad:\n",
    "\t\tW.accumulate_gradient(np.outer(g, x.data))\n",
    "\tif b.require_grad:\n",
    "\t\tb.accumulate_gradient(g)\n",
    "\tif x.require_grad:\n",
    "\t\tx.accumulate_gradient(W.data.T @ g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use an underscore because this function does not manipulate tensors:\n",
    "# it is exactly the same as in the previous exercise\n",
    "def _softmax(x):\n",
    "\tb = np.array(x).max()\n",
    "\ty = np.exp(np.array(x) - b)\n",
    "\treturn y / y.sum()\n",
    "\n",
    "def nll(x, gold):\n",
    "\tb = np.array(x.data).max()\n",
    "\ty = np.exp(np.array(x.data) - b)\n",
    "\tv = -np.log(y[gold] / y.sum())\n",
    "\n",
    "\toutput = Tensor(v, require_grad = True)\n",
    "\toutput.d = backward_nll\n",
    "\toutput.backptr = [x, gold]\n",
    "\n",
    "\treturn output\n",
    "\n",
    "def backward_nll(backptr, g):\n",
    "\tx, gold = backptr\n",
    "\n",
    "\tg_x = _softmax(x.data)\n",
    "\tg_x[gold] -= 1.\n",
    "\n",
    "\tif x.require_grad:\n",
    "\t\tx.accumulate_gradient(g_x * g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module\n",
    "\n",
    "Neural networks or parts of neural networks will be stored in Modules.\n",
    "They implement method to retrieve all parameters of the network and subnetwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\tdef __init__(self):\n",
    "\t\traise NotImplemented(\"\")\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\tret = []\n",
    "\t\tfor name in dir(self):\n",
    "\t\t\to = self.__getattribute__(name)\n",
    "\n",
    "\t\t\tif type(o) is Parameter:\n",
    "\t\t\t\tret.append(o)\n",
    "\t\t\tif isinstance(o, Module) or isinstance(o, ModuleList):\n",
    "\t\t\t\tret.extend(o.parameters())\n",
    "\t\treturn ret\n",
    "\n",
    "# if you want to store a list of Parameters or Module,\n",
    "# you must store them in a ModuleList instead of a python list,\n",
    "# in order to collect the parameters correctly\n",
    "class ModuleList(list):\n",
    "\tdef parameters(self):\n",
    "\t\tret = []\n",
    "\t\tfor m in self:\n",
    "\t\t\tif type(m) is Parameter:\n",
    "\t\t\t\tret.append(m)\n",
    "\t\t\telif isinstance(m, Module) or isinstance(m, ModuleList):\n",
    "\t\t\t\tret.extend(m.parameters())\n",
    "\t\treturn ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(b):\n",
    "\treturn np.zeros(b.shape[0])\n",
    "\n",
    "def glorot_init(W):\n",
    "\trange = np.sqrt(6. / (W.shape[0] + W.shape[1]))\n",
    "\treturn np.random.uniform(-range, range, size = W.shape)\n",
    "\n",
    "def kaiming_init(W):\n",
    "\trange = np.sqrt(6. / W.shape[1])\n",
    "\treturn np.random.uniform(-range, range, size = W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple gradient descent optimizer\n",
    "class SGD:\n",
    "\tdef __init__(self, params, lr, momentum):\n",
    "\t\tself.params = params\n",
    "\t\tself.lr = lr\n",
    "\t\tself.momentum = momentum\n",
    "\n",
    "\tdef step(self):\n",
    "\t\tfor p in self.params:\n",
    "\t\t\tp.velocity = (self.momentum * p.velocity) + ((1. - self.momentum) * p.gradient)\n",
    "\t\t\tp.data[:] = p.data - self.lr * p.velocity\n",
    "\n",
    "\tdef zero_grad(self):\n",
    "\t\tfor p in self.params:\n",
    "\t\t\tp.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks and training loop\n",
    "\n",
    "We first create a simple linear classifier, similar to the first lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(Module):\n",
    "\tdef __init__(self, dim_input, dim_output):\n",
    "\t\t# build the parameters\n",
    "\t\tself.W = Parameter(np.ndarray((dim_output, dim_input)))\n",
    "\t\tself.b = Parameter(np.ndarray((dim_output,)))\n",
    "\n",
    "\t\tself.init_parameters()\n",
    "\n",
    "\tdef init_parameters(self):\n",
    "\t\t# init parameters of the network (i.e W and b)\n",
    "\t\tself.W.data = glorot_init(self.W.data)\n",
    "\t\tself.b.data = zero_init(self.b.data)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn affine_transform(self.W, self.b, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train several neural networks.\n",
    "Therefore, we encapsulate the training loop in a function.\n",
    "\n",
    "**warning**: you have to call optimizer.zero_grad() before each backward pass to reinitialize the gradient of the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDimDataset(data):\n",
    "\tnb_data = data[0].shape[0]\n",
    "\tnb_feature = data[0].shape[1]\n",
    "\tnb_label = len(set(data[1]))\n",
    "\treturn nb_data, nb_feature, nb_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(data):\n",
    "\tindex_list = np.array([i for i in range(len(data[0]))])\n",
    "\tnp.random.shuffle(index_list)\n",
    "\tdata[0] = data[0][index_list]\n",
    "\tdata[1] = data[1][index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network, data):\n",
    "\n",
    "\tloss_sum = 0.\n",
    "\tnb_success = 0\n",
    "\tnb_data, nb_feature, nb_label = getDimDataset(data)\n",
    "\tx = data[0]\n",
    "\tgold = data[1]\n",
    "\n",
    "\tfor i in range(nb_data):\n",
    "\n",
    "\t\ty = network.forward(Tensor(x[i], require_grad = False))\n",
    "\t\to = _softmax(y.data)\n",
    "\n",
    "\t\tloss_sum += nll(y, gold[i]).data\n",
    "\n",
    "\t\tif o.argmax() == gold[i]:\n",
    "\t\t\tnb_success += 1\n",
    "\n",
    "\treturn nb_success / nb_data, loss_sum / nb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, optimizer, train_data, dev_data, epochs, batch_size):\n",
    "\n",
    "\tnb_data, nb_feature, nb_label = getDimDataset(train_data)\n",
    "\tbest_dev = 100000.\n",
    "\tbest_network = deepcopy(network)\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\n",
    "\t\tloss_sum = 0\n",
    "\t\tshuffle(train_data)\n",
    "\t\tx = train_data[0]\n",
    "\t\tgold = train_data[1]\n",
    "\n",
    "\t\tfor batch in range(int(nb_data / batch_size)):\n",
    "\n",
    "\t\t\tloss_sum = 0.\n",
    "\t\t\tnb_success = 0\n",
    "\n",
    "\t\t\tfor i in range(batch * batch_size, (batch + 1) * batch_size):\n",
    "\n",
    "\t\t\t\t# Forward\n",
    "\t\t\t\ty = network.forward(Tensor(x[i], require_grad = False))\n",
    "\t\t\t\to = _softmax(y.data)\n",
    "\t\t\t\tloss = nll(y, gold[i])\n",
    "\t\t\t\tloss_sum += loss.data\n",
    "\n",
    "\t\t\t\tif o.argmax() == gold[i]:\n",
    "\t\t\t\t\tnb_success += 1\n",
    "\n",
    "\t\t\t\t# Backward\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tloss.backward(1.)\n",
    "\n",
    "\t\t\t\t# Update\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# Tests\n",
    "\t\t\tloss = loss_sum / batch_size\n",
    "\t\t\taccuracy = nb_success / batch_size\n",
    "\t\t\tdev_accuracy, dev_loss = test(network, dev_data)\n",
    "\n",
    "\t\t\t# Save the best model\n",
    "\t\t\tif dev_loss < best_dev :\n",
    "\t\t\t\tbest_network = deepcopy(network)\n",
    "\t\t\t\tbest_dev = dev_loss\n",
    "\n",
    "\t\t\t# Print\n",
    "\t\t\tmsg = \"Epoch %i | batch %i | train loss: %.2f | train accuracy: %.1f%% | dev loss: %.2f | dev accuracy: %.1f%%\" % (epoch + 1, batch + 1, loss, accuracy * 100., dev_loss, dev_accuracy * 100.)\n",
    "\t\t\tif batch == int(nb_data / batch_size) - 1: print(msg)\n",
    "\t\t\telse: print(msg, end = \"\\r\")\n",
    "\n",
    "\treturn best_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "Epoch 1 | batch 10 | train loss: 0.33 | train accuracy: 90.4% | dev loss: 0.29 | dev accuracy: 91.8%\n",
      "Epoch 2 | batch 10 | train loss: 0.33 | train accuracy: 90.9% | dev loss: 0.29 | dev accuracy: 92.0%\n",
      "Epoch 3 | batch 10 | train loss: 0.28 | train accuracy: 91.7% | dev loss: 0.28 | dev accuracy: 91.9%\n",
      "Epoch 4 | batch 10 | train loss: 0.28 | train accuracy: 91.9% | dev loss: 0.27 | dev accuracy: 92.4%\n",
      "Epoch 5 | batch 10 | train loss: 0.29 | train accuracy: 92.0% | dev loss: 0.27 | dev accuracy: 92.4%\n",
      "Epoch 6 | batch 10 | train loss: 0.31 | train accuracy: 91.9% | dev loss: 0.28 | dev accuracy: 92.4%\n",
      "Epoch 7 | batch 10 | train loss: 0.27 | train accuracy: 92.2% | dev loss: 0.28 | dev accuracy: 92.3%\n",
      "Epoch 8 | batch 10 | train loss: 0.29 | train accuracy: 91.3% | dev loss: 0.28 | dev accuracy: 92.0%\n",
      "Epoch 9 | batch 10 | train loss: 0.29 | train accuracy: 91.8% | dev loss: 0.27 | dev accuracy: 92.6%\n",
      "Epoch 10 | batch 10 | train loss: 0.26 | train accuracy: 92.4% | dev loss: 0.27 | dev accuracy: 92.6%\n",
      "\n",
      "Testing...\n",
      "\n",
      "Test accuracy: 92.4%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 5000\n",
    "nb_data, nb_feature, nb_label = getDimDataset(train_data)\n",
    "\n",
    "network = LinearNetwork(nb_feature, nb_label)\n",
    "optimizer = SGD(network.parameters(), 0.01, 0.9)\n",
    "\n",
    "print(\"Training...\\n\")\n",
    "network = train(network, optimizer, train_data, dev_data, epochs, batch_size)\n",
    "\n",
    "print(\"\\nTesting...\\n\")\n",
    "test_accuracy, test_loss = test(network, test_data)\n",
    "\n",
    "print(\"Test accuracy: %.1f%%\" % (test_accuracy * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you finished the linear network, you can move to a deep network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(Module):\n",
    "\tdef __init__(self, dim_input, dim_output, hidden_dim, n_layers, tanh = False):\n",
    "\t\tself.W = ModuleList()\n",
    "\t\tself.b = ModuleList()\n",
    "\t\tself.n_layers = n_layers\n",
    "\t\tself.tanh = tanh\n",
    "\n",
    "\t\tfor layer in range(n_layers):\n",
    "\n",
    "\t\t\tif layer == 0:\n",
    "\t\t\t\tweights = Parameter(np.zeros((hidden_dim, dim_input)))\n",
    "\t\t\telse:\n",
    "\t\t\t\tweights = Parameter(np.zeros((hidden_dim, hidden_dim)))\n",
    "\n",
    "\t\t\tbias = Parameter(np.zeros((hidden_dim,)))\n",
    "\n",
    "\t\t\tself.W.append(weights)\n",
    "\t\t\tself.b.append(bias)\n",
    "\n",
    "\t\tself.output_proj = Parameter(np.zeros((dim_output, hidden_dim)))\n",
    "\t\tself.output_bias = Parameter(np.zeros((dim_output,)))\n",
    "\n",
    "\t\tself.init_parameters()\n",
    "\n",
    "\tdef init_parameters(self):\n",
    "\n",
    "\t\tfor weight in self.W.parameters():\n",
    "\t\t\tif self.tanh:\n",
    "\t\t\t\tweight.data = glorot_init(weight.data)\n",
    "\t\t\telse:\n",
    "\t\t\t\tweight.data = kaiming_init(weight.data)\n",
    "\n",
    "\t\tfor bias in self.b.parameters():\n",
    "\t\t\tif self.tanh:\n",
    "\t\t\t\tbias.data = zero_init(bias.data)\n",
    "\t\t\telse:\n",
    "\t\t\t\tbias.data = zero_init(bias.data) + 0.01\n",
    "\n",
    "\t\tif self.tanh:\n",
    "\t\t\tself.output_proj.data = glorot_init(self.output_proj.data)\n",
    "\t\telse:\n",
    "\t\t\tself.output_proj.data = kaiming_init(self.output_proj.data)\n",
    "\n",
    "\t\tif self.tanh:\n",
    "\t\t\tself.output_bias.data = zero_init(self.output_bias.data)\n",
    "\t\telse:\n",
    "\t\t\tself.output_bias.data = zero_init(self.output_bias.data) + 0.01\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tfor i in range(self.n_layers):\n",
    "\n",
    "\t\t\tx = affine_transform(self.W.parameters()[i], self.b.parameters()[i], x)\n",
    "\n",
    "\t\t\tif self.tanh:\n",
    "\t\t\t\tx = tanh(x)\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = relu(x)\n",
    "\n",
    "\t\treturn affine_transform(self.output_proj, self.output_bias, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "Epoch 1 | batch 5 | train loss: 0.15 | train accuracy: 95.5% | dev loss: 0.12 | dev accuracy: 96.5%\n",
      "Epoch 2 | batch 5 | train loss: 0.10 | train accuracy: 97.3% | dev loss: 0.10 | dev accuracy: 96.9%\n",
      "Epoch 3 | batch 5 | train loss: 0.08 | train accuracy: 97.5% | dev loss: 0.09 | dev accuracy: 97.1%\n",
      "Epoch 4 | batch 5 | train loss: 0.06 | train accuracy: 98.1% | dev loss: 0.09 | dev accuracy: 97.2%\n",
      "Epoch 5 | batch 5 | train loss: 0.05 | train accuracy: 98.4% | dev loss: 0.09 | dev accuracy: 97.3%\n",
      "Epoch 6 | batch 5 | train loss: 0.04 | train accuracy: 98.7% | dev loss: 0.09 | dev accuracy: 97.6%\n",
      "Epoch 7 | batch 5 | train loss: 0.03 | train accuracy: 99.1% | dev loss: 0.09 | dev accuracy: 97.5%\n",
      "Epoch 8 | batch 5 | train loss: 0.02 | train accuracy: 99.3% | dev loss: 0.10 | dev accuracy: 97.5%\n",
      "Epoch 9 | batch 5 | train loss: 0.02 | train accuracy: 99.4% | dev loss: 0.09 | dev accuracy: 97.8%\n",
      "Epoch 10 | batch 5 | train loss: 0.01 | train accuracy: 99.6% | dev loss: 0.09 | dev accuracy: 97.6%\n",
      "Epoch 11 | batch 5 | train loss: 0.01 | train accuracy: 99.8% | dev loss: 0.09 | dev accuracy: 97.8%\n",
      "Epoch 12 | batch 5 | train loss: 0.01 | train accuracy: 99.9% | dev loss: 0.09 | dev accuracy: 97.9%\n",
      "Epoch 13 | batch 5 | train loss: 0.00 | train accuracy: 100.0% | dev loss: 0.09 | dev accuracy: 97.9%\n",
      "Epoch 14 | batch 5 | train loss: 0.00 | train accuracy: 100.0% | dev loss: 0.09 | dev accuracy: 97.9%\n",
      "Epoch 15 | batch 5 | train loss: 0.00 | train accuracy: 100.0% | dev loss: 0.09 | dev accuracy: 98.0%\n",
      "Epoch 16 | batch 5 | train loss: 0.00 | train accuracy: 100.0% | dev loss: 0.09 | dev accuracy: 98.0%\n",
      "Epoch 17 | batch 5 | train loss: 0.00 | train accuracy: 100.0% | dev loss: 0.09 | dev accuracy: 98.0%\n",
      "Epoch 18 | batch 5 | train loss: 0.00 | train accuracy: 100.0% | dev loss: 0.09 | dev accuracy: 98.0%\n",
      "Epoch 19 | batch 5 | train loss: 0.00 | train accuracy: 100.0% | dev loss: 0.09 | dev accuracy: 98.0%\n",
      "Epoch 20 | batch 5 | train loss: 0.00 | train accuracy: 100.0% | dev loss: 0.09 | dev accuracy: 98.0%\n",
      "\n",
      "Testing...\n",
      "\n",
      "Test accuracy: 98.0%\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 10000\n",
    "nb_data, nb_feature, nb_label = getDimDataset(train_data)\n",
    "\n",
    "network = DeepNetwork(nb_feature, nb_label, 128, 1, tanh = False)\n",
    "optimizer = SGD(network.parameters(), 0.01, 0.9)\n",
    "\n",
    "print(\"Training...\\n\")\n",
    "network = train(network, optimizer, train_data, dev_data, epochs, batch_size)\n",
    "\n",
    "print(\"\\nTesting...\\n\")\n",
    "test_accuracy, test_loss = test(network, test_data)\n",
    "\n",
    "print(\"Test accuracy: %.1f%%\" % (test_accuracy * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "You can try to implement a momentum SGD optimizer! Note that you have to keep track of the velocity for each parameter in the optimizer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
