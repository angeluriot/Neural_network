{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(weights):\n",
    "\trange = np.sqrt(6. / (weights[0].shape[0] + weights[0].shape[1]))\n",
    "\tweights[0] = np.random.uniform(-range, range, size = weights[0].shape)\n",
    "\tweights[1] = np.zeros(weights[1].shape)\n",
    "\n",
    "def kaiming_init(weights):\n",
    "\trange = np.sqrt(6. / weights[0].shape[1])\n",
    "\tweights[0] = np.random.uniform(-range, range, size = weights[0].shape)\n",
    "\tweights[1] = np.zeros(weights[1].shape) + 0.01\n",
    "\n",
    "def random_init(weights):\n",
    "\tweights[0] = np.random.uniform(-1., 1., size = weights[0].shape)\n",
    "\tweights[1] = np.zeros(weights[1].shape)\n",
    "\n",
    "def shuffle(x, y):\n",
    "\tindex_list = np.array([i for i in range(x.shape[0])])\n",
    "\tnp.random.shuffle(index_list)\n",
    "\tx = x[index_list]\n",
    "\ty = y[index_list]\n",
    "\n",
    "def flatten(x):\n",
    "\treturn x.reshape(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "\tdef create(self, input_size):\n",
    "\t\tpass\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tpass\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "\n",
    "\tdef create(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.output_size = input_size\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.input = input\n",
    "\t\treturn np.maximum(0, input)\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\treturn gradient * (self.input > 0)\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "\n",
    "\tdef create(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.output_size = input_size\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.input = input\n",
    "\t\treturn np.tanh(input)\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\treturn gradient * (1 - np.tanh(self.input) ** 2)\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "\n",
    "\tdef create(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.output_size = input_size\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.input = input\n",
    "\t\tb = input.max()\n",
    "\t\te = np.exp(input - b)\n",
    "\t\treturn e / e.sum()\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\treturn gradient.copy()\n",
    "\n",
    "\n",
    "class Input(Layer):\n",
    "\n",
    "\tdef __init__(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.output_size = input_size\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn input.copy()\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\treturn gradient.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Layer):\n",
    "\n",
    "\tdef init(self, next_layer):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "class Linear(Parameter):\n",
    "\n",
    "\tdef __init__(self, nb_neurons):\n",
    "\t\tself.output_size = nb_neurons\n",
    "\n",
    "\tdef create(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.weights = [np.zeros((self.output_size, input_size)), np.zeros((self.output_size))]\n",
    "\t\tself.velocities = [0, 0]\n",
    "\t\tself.gradients = [np.zeros((self.output_size, input_size)), np.zeros((self.output_size))]\n",
    "\n",
    "\tdef init(self, next_layer):\n",
    "\t\tif type(next_layer) == ReLU:\n",
    "\t\t\tkaiming_init(self.weights)\n",
    "\t\telse:\n",
    "\t\t\tglorot_init(self.weights)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.input = input\n",
    "\t\treturn self.weights[0] @ input + self.weights[1]\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\tself.gradients[0] += np.outer(gradient, self.input)\n",
    "\t\tself.gradients[1] += gradient\n",
    "\t\treturn self.weights[0].T @ gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "\n",
    "\tdef __init__(self, learning_rate):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\n",
    "\tdef update(self, layers):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "\n",
    "\tdef __init__(self, learning_rate, momentum):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.momentum = momentum\n",
    "\n",
    "\tdef update(self, layers):\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tif isinstance(layer, Parameter):\n",
    "\t\t\t\tfor i in range(len(layer.weights)):\n",
    "\t\t\t\t\tlayer.velocities[i] = (self.momentum * layer.velocities[i]) + ((1. - self.momentum) * layer.gradients[i])\n",
    "\t\t\t\t\tlayer.weights[i] -= layer.velocities[i] * self.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef forward(self, output, target):\n",
    "\t\tpass\n",
    "\n",
    "\tdef backward(self, output, target):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "class NegativeLogLikelihood(Loss):\n",
    "\n",
    "\tdef forward(self, output, target):\n",
    "\t\treturn -np.log(output[target])\n",
    "\n",
    "\tdef backward(self, output, target):\n",
    "\t\tgradient = output.copy()\n",
    "\t\tgradient[target] -= 1\n",
    "\t\treturn gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.layers = []\n",
    "\n",
    "\tdef add(self, layer):\n",
    "\n",
    "\t\tif not isinstance(layer, Layer):\n",
    "\t\t\traise TypeError(\"Layer must be an instance of Layer\")\n",
    "\n",
    "\t\tif len(self.layers) > 0:\n",
    "\t\t\tif type(layer) == Input:\n",
    "\t\t\t\traise Exception(\"Input layer cannot be added after other layers\")\n",
    "\t\t\tlayer.create(self.layers[-1].output_size)\n",
    "\t\telif type(layer) != Input:\n",
    "\t\t\traise Exception(\"First layer must be an input layer\")\n",
    "\t\telse:\n",
    "\t\t\tself.input_size = layer.input_size\n",
    "\n",
    "\t\tself.layers.append(layer)\n",
    "\n",
    "\tdef compile(self, loss, optimizer):\n",
    "\n",
    "\t\tif not isinstance(optimizer, Optimizer):\n",
    "\t\t\traise TypeError(\"Optimizer must be an instance of Optimizer\")\n",
    "\t\tif not isinstance(loss, Loss):\n",
    "\t\t\traise TypeError(\"Loss must be an instance of Loss\")\n",
    "\n",
    "\t\tself.optimizer = optimizer\n",
    "\t\tself.loss = loss\n",
    "\n",
    "\t\tfor i in range(len(self.layers) - 1):\n",
    "\t\t\tif type(self.layers[i]) == Softmax:\n",
    "\t\t\t\traise Exception(\"Softmax layer must be the last layer\")\n",
    "\t\t\telif isinstance(self.layers[i], Parameter):\n",
    "\t\t\t\tself.layers[i].init(self.layers[i + 1])\n",
    "\n",
    "\t\tif isinstance(self.layers[-1], Parameter):\n",
    "\t\t\tself.layers[-1].init(None)\n",
    "\n",
    "\t\tself.output_size = self.layers[-1].output_size\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tinput = layer.forward(input)\n",
    "\n",
    "\t\treturn input\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\n",
    "\t\tfor layer in reversed(self.layers):\n",
    "\t\t\tgradient = layer.backward(gradient)\n",
    "\n",
    "\tdef clear_gradients(self):\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tif isinstance(layer, Parameter):\n",
    "\t\t\t\tfor gradient in layer.gradients:\n",
    "\t\t\t\t\tgradient[:] = 0\n",
    "\n",
    "\tdef average_gradients(self, batch_size):\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tif isinstance(layer, Parameter):\n",
    "\t\t\t\tfor gradient in layer.gradients:\n",
    "\t\t\t\t\tgradient /= batch_size\n",
    "\n",
    "\tdef check_input(self, x, y):\n",
    "\n",
    "\t\tnb_data = x.shape[0]\n",
    "\n",
    "\t\tif y.shape[0] != nb_data:\n",
    "\t\t\traise Exception(\"Number of labels must be equal to the number of data\")\n",
    "\n",
    "\t\tx_copy = flatten(x.copy())\n",
    "\t\ty_copy = flatten(y.copy())\n",
    "\n",
    "\t\tif x_copy.shape[1] != self.input_size:\n",
    "\t\t\traise Exception(\"Features size must be equal to the input size of the model\")\n",
    "\n",
    "\t\tif type(self.loss) == NegativeLogLikelihood:\n",
    "\t\t\tif y_copy.shape[1] != self.output_size and y_copy.shape[1] != 1:\n",
    "\t\t\t\traise Exception(\"Labels size must equal to 1 or the output size of the model\")\n",
    "\t\t\tif y_copy.shape[1] == self.output_size:\n",
    "\t\t\t\ty_copy = y_copy.argmax(axis = 1)\n",
    "\t\telse:\n",
    "\t\t\tif y_copy.shape[1] != self.output_size:\n",
    "\t\t\t\traise Exception(\"Labels size must be equal to the output size of the model\")\n",
    "\n",
    "\t\treturn x_copy, y_copy\n",
    "\n",
    "\tdef train(self, x_train, y_train, epochs, batch_size, x_val = None, y_val = None, print_frequency = 1):\n",
    "\n",
    "\t\tx_train_copy, y_train_copy = self.check_input(x_train, y_train)\n",
    "\n",
    "\t\tif x_val is not None:\n",
    "\t\t\tx_val_copy, y_val_copy = self.check_input(x_val, y_val)\n",
    "\n",
    "\t\tnb_data = x_train_copy.shape[0]\n",
    "\t\tbest_dev = 0\n",
    "\t\tbest_model = deepcopy(self)\n",
    "\n",
    "\t\tfor epoch in range(epochs):\n",
    "\n",
    "\t\t\tshuffle(x_train_copy, y_train_copy)\n",
    "\n",
    "\t\t\tfor batch in range(math.floor(nb_data / batch_size)):\n",
    "\n",
    "\t\t\t\tloss = 0\n",
    "\t\t\t\taccuracy = 0\n",
    "\t\t\t\tself.clear_gradients()\n",
    "\n",
    "\t\t\t\tfor i in range(batch * batch_size, (batch + 1) * batch_size):\n",
    "\n",
    "\t\t\t\t\t# Forward\n",
    "\t\t\t\t\toutput = self.forward(x_train_copy[i])\n",
    "\t\t\t\t\tloss += self.loss.forward(output, y_train_copy[i])\n",
    "\n",
    "\t\t\t\t\tif output.argmax() == y_train_copy[i]:\n",
    "\t\t\t\t\t\taccuracy += 1\n",
    "\n",
    "\t\t\t\t\t# Backward\n",
    "\t\t\t\t\tgradient = self.loss.backward(output, y_train_copy[i])\n",
    "\t\t\t\t\tself.backward(gradient)\n",
    "\n",
    "\t\t\t\t# Update\n",
    "\t\t\t\tself.average_gradients(batch_size)\n",
    "\t\t\t\tself.optimizer.update(self.layers)\n",
    "\n",
    "\t\t\t\t# Tests\n",
    "\t\t\t\tif (batch + 1) % print_frequency == 0:\n",
    "\n",
    "\t\t\t\t\tloss /= batch_size\n",
    "\t\t\t\t\taccuracy /= batch_size\n",
    "\n",
    "\t\t\t\t\tif x_val_copy is not None:\n",
    "\n",
    "\t\t\t\t\t\tval_accuracy, val_loss = self.test(x_val_copy, y_val_copy, False, False)\n",
    "\n",
    "\t\t\t\t\t\t# Save the best model\n",
    "\t\t\t\t\t\tif val_accuracy > best_dev :\n",
    "\t\t\t\t\t\t\tbest_model = deepcopy(self)\n",
    "\t\t\t\t\t\t\tbest_dev = val_accuracy\n",
    "\n",
    "\t\t\t\t\t\tmsg = \"Epoch %i | batch %i | train loss: %.2f | train accuracy: %.1f%% | dev loss: %.2f | dev accuracy: %.1f%%\" % (epoch + 1, batch + 1, loss, accuracy * 100., val_loss, val_accuracy * 100.)\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tmsg = \"Epoch %i | batch %i | train loss: %.2f | train accuracy: %.1f%%\" % (epoch + 1, batch + 1, loss, accuracy * 100.)\n",
    "\n",
    "\t\t\t\t\tif batch == int(nb_data / batch_size) - 1: print(msg)\n",
    "\t\t\t\t\telse: print(msg, end = \"\\r\")\n",
    "\n",
    "\t\tif x_val_copy is not None:\n",
    "\t\t\tself.layers = best_model.layers\n",
    "\n",
    "\tdef predict(self, x):\n",
    "\t\toutput = self.forward(x)\n",
    "\t\treturn output.argmax()\n",
    "\n",
    "\tdef test(self, x, y, check_data = True, print_results = True):\n",
    "\n",
    "\t\tif check_data:\n",
    "\t\t\tx_copy, y_copy = self.check_input(x, y)\n",
    "\t\telse:\n",
    "\t\t\tx_copy, y_copy = x, y\n",
    "\n",
    "\t\tnb_data = x.shape[0]\n",
    "\t\tloss = 0\n",
    "\t\taccuracy = 0\n",
    "\n",
    "\t\tfor i in range(nb_data):\n",
    "\n",
    "\t\t\toutput = self.forward(x_copy[i])\n",
    "\t\t\tloss += self.loss.forward(output, y_copy[i])\n",
    "\n",
    "\t\t\tif output.argmax() == y_copy[i]:\n",
    "\t\t\t\taccuracy += 1\n",
    "\n",
    "\t\tif print_results:\n",
    "\t\t\tprint(\"Test loss: %.2f | test accuracy: %.1f%%\" % (loss / nb_data, (accuracy / nb_data) * 100.))\n",
    "\n",
    "\t\treturn accuracy / nb_data, loss / nb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dataset_loader\n",
    "\n",
    "# Download mnist dataset\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "\t# this link doesn't work any more,\n",
    "\t# seach on google for the file \"mnist.pkl.gz\"\n",
    "\t# and download it\n",
    "\t!wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\"\n",
    "\n",
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_data[0])\n",
    "y_train = np.array(train_data[1])\n",
    "x_dev = np.array(dev_data[0])\n",
    "y_dev = np.array(dev_data[1])\n",
    "x_test = np.array(test_data[0])\n",
    "y_test = np.array(test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.add(Input(28 * 28))\n",
    "model.add(Linear(128))\n",
    "model.add(ReLU())\n",
    "model.add(Linear(128))\n",
    "model.add(ReLU())\n",
    "model.add(Linear(10))\n",
    "model.add(ReLU())\n",
    "model.add(Softmax())\n",
    "model.compile(NegativeLogLikelihood(), SGD(0.01, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | batch 500 | train loss: 0.18 | train accuracy: 95.0% | dev loss: 0.20 | dev accuracy: 94.6%\n",
      "Epoch 2 | batch 500 | train loss: 0.17 | train accuracy: 95.0% | dev loss: 0.19 | dev accuracy: 94.7%\n",
      "Epoch 3 | batch 500 | train loss: 0.17 | train accuracy: 95.0% | dev loss: 0.19 | dev accuracy: 94.9%\n",
      "Epoch 4 | batch 500 | train loss: 0.16 | train accuracy: 95.0% | dev loss: 0.18 | dev accuracy: 95.0%\n",
      "Epoch 5 | batch 500 | train loss: 0.16 | train accuracy: 95.0% | dev loss: 0.18 | dev accuracy: 95.2%\n",
      "Epoch 6 | batch 500 | train loss: 0.15 | train accuracy: 95.0% | dev loss: 0.17 | dev accuracy: 95.2%\n",
      "Epoch 7 | batch 500 | train loss: 0.15 | train accuracy: 95.0% | dev loss: 0.17 | dev accuracy: 95.4%\n",
      "Epoch 8 | batch 500 | train loss: 0.15 | train accuracy: 96.0% | dev loss: 0.16 | dev accuracy: 95.5%\n",
      "Epoch 9 | batch 500 | train loss: 0.14 | train accuracy: 96.0% | dev loss: 0.16 | dev accuracy: 95.6%\n",
      "Epoch 10 | batch 500 | train loss: 0.14 | train accuracy: 97.0% | dev loss: 0.16 | dev accuracy: 95.7%\n",
      "Epoch 11 | batch 500 | train loss: 0.14 | train accuracy: 97.0% | dev loss: 0.15 | dev accuracy: 95.8%\n",
      "Epoch 12 | batch 500 | train loss: 0.13 | train accuracy: 97.0% | dev loss: 0.15 | dev accuracy: 95.9%\n",
      "Epoch 13 | batch 500 | train loss: 0.13 | train accuracy: 97.0% | dev loss: 0.15 | dev accuracy: 95.9%\n",
      "Epoch 14 | batch 500 | train loss: 0.13 | train accuracy: 97.0% | dev loss: 0.15 | dev accuracy: 96.0%\n",
      "Epoch 15 | batch 500 | train loss: 0.13 | train accuracy: 97.0% | dev loss: 0.14 | dev accuracy: 96.0%\n",
      "Epoch 16 | batch 500 | train loss: 0.12 | train accuracy: 97.0% | dev loss: 0.14 | dev accuracy: 96.1%\n",
      "Epoch 17 | batch 500 | train loss: 0.12 | train accuracy: 97.0% | dev loss: 0.14 | dev accuracy: 96.1%\n",
      "Epoch 18 | batch 500 | train loss: 0.12 | train accuracy: 97.0% | dev loss: 0.14 | dev accuracy: 96.3%\n",
      "Epoch 19 | batch 500 | train loss: 0.12 | train accuracy: 97.0% | dev loss: 0.13 | dev accuracy: 96.3%\n",
      "Epoch 20 | batch 500 | train loss: 0.11 | train accuracy: 97.0% | dev loss: 0.13 | dev accuracy: 96.4%\n",
      "Epoch 21 | batch 500 | train loss: 0.11 | train accuracy: 97.0% | dev loss: 0.13 | dev accuracy: 96.4%\n",
      "Epoch 22 | batch 500 | train loss: 0.11 | train accuracy: 97.0% | dev loss: 0.13 | dev accuracy: 96.4%\n",
      "Epoch 23 | batch 500 | train loss: 0.11 | train accuracy: 98.0% | dev loss: 0.13 | dev accuracy: 96.5%\n",
      "Epoch 24 | batch 500 | train loss: 0.10 | train accuracy: 98.0% | dev loss: 0.13 | dev accuracy: 96.5%\n",
      "Epoch 25 | batch 500 | train loss: 0.10 | train accuracy: 98.0% | dev loss: 0.13 | dev accuracy: 96.5%\n",
      "Epoch 26 | batch 500 | train loss: 0.10 | train accuracy: 98.0% | dev loss: 0.12 | dev accuracy: 96.5%\n",
      "Epoch 27 | batch 500 | train loss: 0.10 | train accuracy: 98.0% | dev loss: 0.12 | dev accuracy: 96.6%\n",
      "Epoch 28 | batch 500 | train loss: 0.09 | train accuracy: 98.0% | dev loss: 0.12 | dev accuracy: 96.6%\n",
      "Epoch 29 | batch 500 | train loss: 0.09 | train accuracy: 98.0% | dev loss: 0.12 | dev accuracy: 96.6%\n",
      "Epoch 30 | batch 500 | train loss: 0.09 | train accuracy: 98.0% | dev loss: 0.12 | dev accuracy: 96.6%\n",
      "Epoch 31 | batch 500 | train loss: 0.09 | train accuracy: 98.0% | dev loss: 0.12 | dev accuracy: 96.7%\n",
      "Epoch 32 | batch 500 | train loss: 0.09 | train accuracy: 98.0% | dev loss: 0.12 | dev accuracy: 96.7%\n",
      "Epoch 33 | batch 500 | train loss: 0.08 | train accuracy: 98.0% | dev loss: 0.12 | dev accuracy: 96.7%\n",
      "Epoch 34 | batch 500 | train loss: 0.08 | train accuracy: 98.0% | dev loss: 0.11 | dev accuracy: 96.7%\n",
      "Epoch 35 | batch 500 | train loss: 0.08 | train accuracy: 98.0% | dev loss: 0.11 | dev accuracy: 96.7%\n",
      "Epoch 36 | batch 500 | train loss: 0.08 | train accuracy: 98.0% | dev loss: 0.11 | dev accuracy: 96.7%\n",
      "Epoch 37 | batch 500 | train loss: 0.08 | train accuracy: 98.0% | dev loss: 0.11 | dev accuracy: 96.7%\n",
      "Epoch 38 | batch 500 | train loss: 0.07 | train accuracy: 98.0% | dev loss: 0.11 | dev accuracy: 96.7%\n",
      "Epoch 39 | batch 500 | train loss: 0.07 | train accuracy: 98.0% | dev loss: 0.11 | dev accuracy: 96.8%\n",
      "Epoch 40 | batch 500 | train loss: 0.07 | train accuracy: 98.0% | dev loss: 0.11 | dev accuracy: 96.8%\n"
     ]
    }
   ],
   "source": [
    "model.train(x_train, y_train, 10, 100, x_dev, y_dev, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07 | test accuracy: 98.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9802, array([0.07030813]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b4e4b7bb449f191a95e834e491560c409d3dee773b5ecff4cb91991ba727854"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
