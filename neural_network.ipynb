{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(weights):\n",
    "\trange = np.sqrt(6. / (weights[0].shape[0] + weights[0].shape[1]))\n",
    "\tweights[0] = np.random.uniform(-range, range, size = weights[0].shape)\n",
    "\tweights[1] = np.zeros(weights[1].shape)\n",
    "\n",
    "def kaiming_init(weights):\n",
    "\trange = np.sqrt(6. / weights[0].shape[1])\n",
    "\tweights[0] = np.random.uniform(-range, range, size = weights[0].shape)\n",
    "\tweights[1] = np.zeros(weights[1].shape) + 0.01\n",
    "\n",
    "def shuffle(x, y):\n",
    "\tindex_list = np.array([i for i in range(x.shape[0])])\n",
    "\tnp.random.shuffle(index_list)\n",
    "\tx = x[index_list]\n",
    "\ty = y[index_list]\n",
    "\n",
    "def flatten(x):\n",
    "\treturn x.reshape(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "\tdef create(self, input_size):\n",
    "\t\tpass\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tpass\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "\n",
    "\tdef create(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.output_size = input_size\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.input = input\n",
    "\t\treturn np.maximum(0, input)\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\treturn gradient * (self.input > 0)\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "\n",
    "\tdef create(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.output_size = input_size\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.input = input\n",
    "\t\treturn np.tanh(input)\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\treturn gradient * (1 - np.tanh(self.input) ** 2)\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "\n",
    "\tdef create(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.output_size = input_size\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.input = input\n",
    "\t\tb = input.max()\n",
    "\t\te = np.exp(input - b)\n",
    "\t\treturn e / e.sum()\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\treturn gradient.copy()\n",
    "\n",
    "\n",
    "class Input(Layer):\n",
    "\n",
    "\tdef __init__(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.output_size = input_size\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn input.copy()\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\treturn gradient.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Layer):\n",
    "\n",
    "\tdef init(self, next_layer):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "class Linear(Parameter):\n",
    "\n",
    "\tdef __init__(self, nb_neurons):\n",
    "\t\tself.output_size = nb_neurons\n",
    "\n",
    "\tdef create(self, input_size):\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.weights = [np.zeros((self.output_size, input_size)), np.zeros((self.output_size))]\n",
    "\t\tself.velocities = [np.zeros((self.output_size, input_size)), np.zeros((self.output_size))]\n",
    "\t\tself.m = [np.zeros((self.output_size, input_size)), np.zeros((self.output_size))]\n",
    "\t\tself.v = [np.zeros((self.output_size, input_size)), np.zeros((self.output_size))]\n",
    "\t\tself.gradients = [np.zeros((self.output_size, input_size)), np.zeros((self.output_size))]\n",
    "\n",
    "\tdef init(self, next_layer):\n",
    "\t\tif type(next_layer) == ReLU:\n",
    "\t\t\tkaiming_init(self.weights)\n",
    "\t\telse:\n",
    "\t\t\tglorot_init(self.weights)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.input = input\n",
    "\t\treturn self.weights[0] @ input + self.weights[1]\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\t\tself.gradients[0] += np.outer(gradient, self.input)\n",
    "\t\tself.gradients[1] += gradient\n",
    "\t\treturn self.weights[0].T @ gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "\n",
    "\tdef init(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef update(self, layers):\n",
    "\t\tpass\n",
    "\n",
    "\tdef iteration(self):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "\n",
    "\tdef __init__(self, learning_rate = 0.01, momentum = 0.9):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.momentum = momentum\n",
    "\n",
    "\tdef init(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef update(self, layers):\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tif isinstance(layer, Parameter):\n",
    "\t\t\t\tfor i in range(len(layer.weights)):\n",
    "\t\t\t\t\tlayer.velocities[i] = (self.momentum * layer.velocities[i]) + ((1. - self.momentum) * layer.gradients[i])\n",
    "\t\t\t\t\tlayer.weights[i] -= layer.velocities[i] * self.learning_rate\n",
    "\n",
    "\tdef iteration(self):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "\n",
    "\tdef __init__(self, learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-8):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.beta_1 = beta_1\n",
    "\t\tself.beta_2 = beta_2\n",
    "\t\tself.epsilon = epsilon\n",
    "\t\tself.t = 1\n",
    "\n",
    "\tdef init(self):\n",
    "\t\tself.t = 1\n",
    "\n",
    "\tdef update(self, layers):\n",
    "\t\tfor layer in layers:\n",
    "\t\t\tif isinstance(layer, Parameter):\n",
    "\t\t\t\tfor i in range(len(layer.weights)):\n",
    "\t\t\t\t\tlayer.m[i] = self.beta_1 * layer.m[i] + (1. - self.beta_1) * layer.gradients[i]\n",
    "\t\t\t\t\tlayer.v[i] = self.beta_2 * layer.v[i] + (1. - self.beta_2) * layer.gradients[i] ** 2\n",
    "\t\t\t\t\tm_hat = layer.m[i] / (1. - (self.beta_1 ** self.t))\n",
    "\t\t\t\t\tv_hat = layer.v[i] / (1. - (self.beta_2 ** self.t))\n",
    "\t\t\t\t\tlayer.weights[i] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "\tdef iteration(self):\n",
    "\t\tself.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef forward(self, output, target):\n",
    "\t\tpass\n",
    "\n",
    "\tdef backward(self, output, target):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "class NegativeLogLikelihood(Loss):\n",
    "\n",
    "\tdef forward(self, output, target):\n",
    "\t\treturn -np.log(output[target])\n",
    "\n",
    "\tdef backward(self, output, target):\n",
    "\t\tgradient = output.copy()\n",
    "\t\tgradient[target] -= 1\n",
    "\t\treturn gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.layers = []\n",
    "\n",
    "\tdef add(self, layer):\n",
    "\n",
    "\t\tif not isinstance(layer, Layer):\n",
    "\t\t\traise TypeError(\"Layer must be an instance of Layer\")\n",
    "\n",
    "\t\tif len(self.layers) > 0:\n",
    "\t\t\tif type(layer) == Input:\n",
    "\t\t\t\traise Exception(\"Input layer cannot be added after other layers\")\n",
    "\t\t\tlayer.create(self.layers[-1].output_size)\n",
    "\t\telif type(layer) != Input:\n",
    "\t\t\traise Exception(\"First layer must be an input layer\")\n",
    "\t\telse:\n",
    "\t\t\tself.input_size = layer.input_size\n",
    "\n",
    "\t\tself.layers.append(layer)\n",
    "\n",
    "\tdef compile(self, loss, optimizer):\n",
    "\n",
    "\t\tif not isinstance(optimizer, Optimizer):\n",
    "\t\t\traise TypeError(\"Optimizer must be an instance of Optimizer\")\n",
    "\t\tif not isinstance(loss, Loss):\n",
    "\t\t\traise TypeError(\"Loss must be an instance of Loss\")\n",
    "\n",
    "\t\tself.optimizer = optimizer\n",
    "\t\tself.loss = loss\n",
    "\n",
    "\t\tfor i in range(len(self.layers) - 1):\n",
    "\t\t\tif type(self.layers[i]) == Softmax:\n",
    "\t\t\t\traise Exception(\"Softmax layer must be the last layer\")\n",
    "\t\t\telif isinstance(self.layers[i], Parameter):\n",
    "\t\t\t\tself.layers[i].init(self.layers[i + 1])\n",
    "\n",
    "\t\tif isinstance(self.layers[-1], Parameter):\n",
    "\t\t\tself.layers[-1].init(None)\n",
    "\n",
    "\t\tself.output_size = self.layers[-1].output_size\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tinput = layer.forward(input)\n",
    "\n",
    "\t\treturn input\n",
    "\n",
    "\tdef backward(self, gradient):\n",
    "\n",
    "\t\tfor layer in reversed(self.layers):\n",
    "\t\t\tgradient = layer.backward(gradient)\n",
    "\n",
    "\tdef clear_gradients(self):\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tif isinstance(layer, Parameter):\n",
    "\t\t\t\tfor gradient in layer.gradients:\n",
    "\t\t\t\t\tgradient[:] = 0\n",
    "\n",
    "\tdef average_gradients(self, batch_size):\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tif isinstance(layer, Parameter):\n",
    "\t\t\t\tfor gradient in layer.gradients:\n",
    "\t\t\t\t\tgradient /= batch_size\n",
    "\n",
    "\tdef check_input(self, x, y):\n",
    "\n",
    "\t\tnb_data = x.shape[0]\n",
    "\n",
    "\t\tif y.shape[0] != nb_data:\n",
    "\t\t\traise Exception(\"Number of labels must be equal to the number of data\")\n",
    "\n",
    "\t\tx_copy = flatten(x.copy())\n",
    "\t\ty_copy = flatten(y.copy())\n",
    "\n",
    "\t\tif x_copy.shape[1] != self.input_size:\n",
    "\t\t\traise Exception(\"Features size must be equal to the input size of the model\")\n",
    "\n",
    "\t\tif type(self.loss) == NegativeLogLikelihood:\n",
    "\t\t\tif y_copy.shape[1] != self.output_size and y_copy.shape[1] != 1:\n",
    "\t\t\t\traise Exception(\"Labels size must equal to 1 or the output size of the model\")\n",
    "\t\t\tif y_copy.shape[1] == self.output_size:\n",
    "\t\t\t\ty_copy = y_copy.argmax(axis = 1)\n",
    "\t\telse:\n",
    "\t\t\tif y_copy.shape[1] != self.output_size:\n",
    "\t\t\t\traise Exception(\"Labels size must be equal to the output size of the model\")\n",
    "\n",
    "\t\treturn x_copy, y_copy\n",
    "\n",
    "\tdef train(self, x_train, y_train, epochs, batch_size, x_val = None, y_val = None, print_frequency = 1):\n",
    "\n",
    "\t\tx_train_copy, y_train_copy = self.check_input(x_train, y_train)\n",
    "\n",
    "\t\tif x_val is not None:\n",
    "\t\t\tx_val_copy, y_val_copy = self.check_input(x_val, y_val)\n",
    "\n",
    "\t\tnb_data = x_train_copy.shape[0]\n",
    "\t\tbest_dev = 0\n",
    "\t\tbest_model = deepcopy(self)\n",
    "\t\tself.optimizer.init()\n",
    "\n",
    "\t\tfor epoch in range(epochs):\n",
    "\n",
    "\t\t\tshuffle(x_train_copy, y_train_copy)\n",
    "\n",
    "\t\t\tfor batch in range(math.floor(nb_data / batch_size)):\n",
    "\n",
    "\t\t\t\tloss = 0\n",
    "\t\t\t\taccuracy = 0\n",
    "\t\t\t\tself.clear_gradients()\n",
    "\n",
    "\t\t\t\tfor i in range(batch * batch_size, (batch + 1) * batch_size):\n",
    "\n",
    "\t\t\t\t\t# Forward\n",
    "\t\t\t\t\toutput = self.forward(x_train_copy[i])\n",
    "\t\t\t\t\tloss += self.loss.forward(output, y_train_copy[i])\n",
    "\n",
    "\t\t\t\t\tif output.argmax() == y_train_copy[i]:\n",
    "\t\t\t\t\t\taccuracy += 1\n",
    "\n",
    "\t\t\t\t\t# Backward\n",
    "\t\t\t\t\tgradient = self.loss.backward(output, y_train_copy[i])\n",
    "\t\t\t\t\tself.backward(gradient)\n",
    "\n",
    "\t\t\t\t# Update\n",
    "\t\t\t\tself.average_gradients(batch_size)\n",
    "\t\t\t\tself.optimizer.update(self.layers)\n",
    "\n",
    "\t\t\t\t# Tests\n",
    "\t\t\t\tif (batch + 1) % print_frequency == 0:\n",
    "\n",
    "\t\t\t\t\tloss /= batch_size\n",
    "\t\t\t\t\taccuracy /= batch_size\n",
    "\n",
    "\t\t\t\t\tif x_val_copy is not None:\n",
    "\n",
    "\t\t\t\t\t\tval_accuracy, val_loss = self.test(x_val_copy, y_val_copy, False, False)\n",
    "\n",
    "\t\t\t\t\t\t# Save the best model\n",
    "\t\t\t\t\t\tif val_accuracy > best_dev :\n",
    "\t\t\t\t\t\t\tbest_model = deepcopy(self)\n",
    "\t\t\t\t\t\t\tbest_dev = val_accuracy\n",
    "\n",
    "\t\t\t\t\t\tmsg = \"Epoch %i | batch %i | train loss: %.2f | train accuracy: %.1f%% | dev loss: %.2f | dev accuracy: %.1f%%\" % (epoch + 1, batch + 1, loss, accuracy * 100., val_loss, val_accuracy * 100.)\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tmsg = \"Epoch %i | batch %i | train loss: %.2f | train accuracy: %.1f%%\" % (epoch + 1, batch + 1, loss, accuracy * 100.)\n",
    "\n",
    "\t\t\t\t\tif batch == int(nb_data / batch_size) - 1: print(msg)\n",
    "\t\t\t\t\telse: print(msg, end = \"\\r\")\n",
    "\n",
    "\t\t\tself.optimizer.iteration()\n",
    "\n",
    "\t\tif x_val_copy is not None:\n",
    "\t\t\tself.layers = best_model.layers\n",
    "\n",
    "\tdef predict(self, x):\n",
    "\t\toutput = self.forward(x)\n",
    "\t\treturn output.argmax()\n",
    "\n",
    "\tdef test(self, x, y, check_data = True, print_results = True):\n",
    "\n",
    "\t\tif check_data:\n",
    "\t\t\tx_copy, y_copy = self.check_input(x, y)\n",
    "\t\telse:\n",
    "\t\t\tx_copy, y_copy = x, y\n",
    "\n",
    "\t\tnb_data = x.shape[0]\n",
    "\t\tloss = 0\n",
    "\t\taccuracy = 0\n",
    "\n",
    "\t\tfor i in range(nb_data):\n",
    "\n",
    "\t\t\toutput = self.forward(x_copy[i])\n",
    "\t\t\tloss += self.loss.forward(output, y_copy[i])\n",
    "\n",
    "\t\t\tif output.argmax() == y_copy[i]:\n",
    "\t\t\t\taccuracy += 1\n",
    "\n",
    "\t\tif print_results:\n",
    "\t\t\tprint(\"Test loss: %.2f | test accuracy: %.1f%%\" % (loss / nb_data, (accuracy / nb_data) * 100.))\n",
    "\n",
    "\t\treturn accuracy / nb_data, loss / nb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dataset_loader\n",
    "\n",
    "# Download mnist dataset\n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "\t# this link doesn't work any more,\n",
    "\t# seach on google for the file \"mnist.pkl.gz\"\n",
    "\t# and download it\n",
    "\t!wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\"\n",
    "\n",
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_data[0])\n",
    "y_train = np.array(train_data[1])\n",
    "x_dev = np.array(dev_data[0])\n",
    "y_dev = np.array(dev_data[1])\n",
    "x_test = np.array(test_data[0])\n",
    "y_test = np.array(test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.add(Input(28 * 28))\n",
    "model.add(Linear(128))\n",
    "model.add(ReLU())\n",
    "model.add(Linear(10))\n",
    "model.add(ReLU())\n",
    "model.add(Softmax())\n",
    "model.compile(NegativeLogLikelihood(), Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(x_train, y_train, 10, 100, x_dev, y_dev, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b4e4b7bb449f191a95e834e491560c409d3dee773b5ecff4cb91991ba727854"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
